#!/usr/bin/env python3
"""æŸ¥çœ‹å’Œç»Ÿè®¡æ•°æ®é›†çš„å·¥å…·è„šæœ¬"""

import argparse
import json
from pathlib import Path
from typing import List, Dict


def load_dataset(dataset_dir: str) -> List[Dict]:
    """åŠ è½½æ•°æ®é›†"""
    dataset_path = Path(dataset_dir)
    jsonl_path = dataset_path / "dataset.jsonl"
    
    if not jsonl_path.exists():
        print(f"âŒ æ•°æ®é›†ä¸å­˜åœ¨: {jsonl_path}")
        return []
    
    records = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                records.append(json.loads(line))
    
    return records


def show_statistics(records: List[Dict]) -> None:
    """æ˜¾ç¤ºæ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
    if not records:
        print("æ•°æ®é›†ä¸ºç©º")
        return
    
    total_count = len(records)
    total_duration = sum(r["duration"] for r in records)
    avg_duration = total_duration / total_count
    avg_latency = sum(r["inference_latency"] for r in records) / total_count
    avg_confidence = sum(r.get("confidence", 0) for r in records) / total_count
    
    # æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
    text_lengths = [len(r["text"]) for r in records]
    avg_text_length = sum(text_lengths) / total_count
    max_text_length = max(text_lengths)
    min_text_length = min(text_lengths)
    
    print("\n" + "="*60)
    print("ðŸ“Š æ•°æ®é›†ç»Ÿè®¡")
    print("="*60)
    print(f"æ ·æœ¬æ•°é‡:    {total_count:>8} æ¡")
    print(f"æ€»æ—¶é•¿:      {total_duration:>8.1f} ç§’ ({total_duration/60:.1f} åˆ†é’Ÿ)")
    print(f"å¹³å‡æ—¶é•¿:    {avg_duration:>8.2f} ç§’")
    print(f"å¹³å‡æŽ¨ç†:    {avg_latency:>8.2f} ç§’")
    print(f"å¹³å‡ç½®ä¿¡åº¦:  {avg_confidence:>8.2%}")
    print(f"å¹³å‡æ–‡æœ¬é•¿åº¦: {avg_text_length:>7.1f} å­—")
    print(f"æ–‡æœ¬é•¿åº¦èŒƒå›´: {min_text_length:>3} - {max_text_length:>3} å­—")
    print("="*60 + "\n")


def show_samples(records: List[Dict], count: int = 5) -> None:
    """æ˜¾ç¤ºæ ·æœ¬"""
    print(f"\nðŸ“ æœ€è¿‘ {min(count, len(records))} æ¡æ ·æœ¬ï¼š\n")
    
    for i, record in enumerate(records[-count:], 1):
        print(f"[{i}] ID: {record['id']}")
        print(f"    æ–‡æœ¬: {record['text']}")
        print(f"    æ—¶é•¿: {record['duration']:.2f}s | "
              f"ç½®ä¿¡åº¦: {record.get('confidence', 0):.2%} | "
              f"æŽ¨ç†: {record['inference_latency']:.2f}s")
        print(f"    æ—¶é—´: {record['timestamp']}")
        print()


def filter_low_quality(records: List[Dict], 
                       min_confidence: float = 0.8,
                       min_duration: float = 0.5) -> List[Dict]:
    """è¿‡æ»¤ä½Žè´¨é‡æ ·æœ¬"""
    filtered = [
        r for r in records
        if r.get("confidence", 0) >= min_confidence and 
           r["duration"] >= min_duration and
           len(r["text"].strip()) > 0
    ]
    return filtered


def main():
    parser = argparse.ArgumentParser(description="æŸ¥çœ‹æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--dataset-dir", default="dataset", help="æ•°æ®é›†ç›®å½•")
    parser.add_argument("--samples", type=int, default=5, help="æ˜¾ç¤ºæœ€è¿‘Næ¡æ ·æœ¬")
    parser.add_argument("--filter", action="store_true", help="åªæ˜¾ç¤ºé«˜è´¨é‡æ ·æœ¬")
    parser.add_argument("--min-confidence", type=float, default=0.8, help="æœ€å°ç½®ä¿¡åº¦")
    parser.add_argument("--min-duration", type=float, default=0.5, help="æœ€å°æ—¶é•¿ï¼ˆç§’ï¼‰")
    
    args = parser.parse_args()
    
    # åŠ è½½æ•°æ®é›†
    records = load_dataset(args.dataset_dir)
    
    if not records:
        return
    
    print(f"\nðŸ“ æ•°æ®é›†ç›®å½•: {Path(args.dataset_dir).absolute()}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    show_statistics(records)
    
    # å¦‚æžœéœ€è¦è¿‡æ»¤
    if args.filter:
        filtered_records = filter_low_quality(
            records,
            min_confidence=args.min_confidence,
            min_duration=args.min_duration
        )
        print(f"\nðŸ” è¿‡æ»¤åŽæ ·æœ¬æ•°: {len(filtered_records)}/{len(records)} "
              f"({len(filtered_records)/len(records)*100:.1f}%)")
        print(f"   æ¡ä»¶: ç½®ä¿¡åº¦ >= {args.min_confidence:.0%}, "
              f"æ—¶é•¿ >= {args.min_duration:.1f}s")
        records = filtered_records
        
        if records:
            show_statistics(records)
    
    # æ˜¾ç¤ºæ ·æœ¬
    if args.samples > 0:
        show_samples(records, args.samples)


if __name__ == "__main__":
    main()

